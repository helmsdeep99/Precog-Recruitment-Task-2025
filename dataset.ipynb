{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ac0d19-7b74-484b-bf44-ee1b32574cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad769e13-5969-4d76-bdbd-b0920ece3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountryLoader:\n",
    "    # Class to load and process United Nations Member States\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://www.un.org/en/about-us/member-states'\n",
    "\n",
    "        # Set up logger to log messages for various events and errors\n",
    "        logging.basicConfig(level=logging.INFO) # Log messages with a security level of INFO or higher\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def fetch_and_parse(self, url: str) -> BeautifulSoup:\n",
    "        # Fetch content from the URL and return parsed BeautifulSoup object\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            self.logger.error(f'Failed to fetch URL {url}: {str(e)}')\n",
    "            raise\n",
    "\n",
    "    def extract_countries(self, soup: BeautifulSoup): # -> List[str]\n",
    "        # Extract country names from parsed HTML\n",
    "        countries = []\n",
    "\n",
    "        try:\n",
    "            # 'mb-2' is a unique CSS class, not present elsewhere in the HTML\n",
    "            # This div contains the countries inside 'col-md-12' divs\n",
    "            # Names are contained in h2 elements with class 'mb-0'\n",
    "            block = soup.find('div', class_='mb-2') # Works as of 22nd January 2025\n",
    "\n",
    "            if block is None:\n",
    "                self.logger.error(f'No div with class \"mb-2\" found, URL {url} structure has likely been changed\"')\n",
    "                                            \n",
    "            for country in block.find_all('h2', class_='mb-0'): # Works as of 22nd Janaury 2025\n",
    "                name = country.text.strip()\n",
    "                if name:\n",
    "                    countries.append(name)\n",
    "\n",
    "            # Check if the countries list is populated\n",
    "            if not countries:\n",
    "                self.logger.warning('No country names found in \"mb-2\" block')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'An error occured while extracting country names: {str(e)}')\n",
    "            raise\n",
    "\n",
    "        return countries\n",
    "\n",
    "    def clean_country_name(self, name: str) -> str:\n",
    "        # Standardizing country names for clarity and consistency, where official names are more commonly referred to by other names in international contexts.\n",
    "        name_mapping = {\n",
    "            'Democratic People\\'s Republic of Korea': 'North Korea',\n",
    "            'Democratic Republic of the Congo': 'DR Congo',\n",
    "            'Lao People’s Democratic Republic': 'Laos',\n",
    "            'Republic of Korea': 'South Korea',\n",
    "            'Republic of Moldova': 'Moldova',\n",
    "            'Russian Federation': 'Russia',\n",
    "            'Syrian Arab Republic': 'Syria',\n",
    "            'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom',\n",
    "            'United Republic of Tanzania': 'Tanzania',\n",
    "        }\n",
    "\n",
    "        if name in name_mapping:\n",
    "            name = name_mapping[name] # Will change this later to generalize to lower case and other stuff\n",
    "\n",
    "        # Some country names have official designations in brackets\n",
    "        # Other country names like Venezuela have designations after a comma\n",
    "        # Remove these since they aren't relevant in the game\n",
    "\n",
    "        # Match sequence of characters in paranthesis and remove it\n",
    "        name = re.sub(r'\\([^)]*\\)', '', name)\n",
    "\n",
    "        # Remove everything after (and incluing) a comma\n",
    "        name = re.sub(r',.*', '', name)\n",
    "\n",
    "        return name.strip()\n",
    "\n",
    "    def load_countries(self): # -> List[str]\n",
    "        # Main method to load and process country data\n",
    "        try:\n",
    "            soup = self.fetch_and_parse(self.base_url)\n",
    "            countries = self.extract_countries(soup)\n",
    "            countries = [self.clean_country_name(country) for country in countries]\n",
    "\n",
    "            return countries\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Failed to load countries: {str(e)}')\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27ee822-e7aa-4057-ae33-aa6cfe70feed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 192 countries\n",
      "Afghanistan\n",
      "Albania\n",
      "Algeria\n",
      "Andorra\n",
      "Angola\n",
      "Antigua and Barbuda\n",
      "Argentina\n",
      "Armenia\n",
      "Australia\n",
      "Austria\n",
      "Azerbaijan\n",
      "Bahamas\n",
      "Bahrain\n",
      "Bangladesh\n",
      "Barbados\n",
      "Belarus\n",
      "Belgium\n",
      "Belize\n",
      "Benin\n",
      "Bhutan\n",
      "Bolivia\n",
      "Bosnia and Herzegovina\n",
      "Botswana\n",
      "Brazil\n",
      "Brunei Darussalam\n",
      "Bulgaria\n",
      "Burkina Faso\n",
      "Burundi\n",
      "Cabo Verde\n",
      "Cambodia\n",
      "Cameroon\n",
      "Canada\n",
      "Central African Republic\n",
      "Chad\n",
      "Chile\n",
      "China\n",
      "Colombia\n",
      "Comoros\n",
      "Congo\n",
      "Costa Rica\n",
      "Croatia\n",
      "Cuba\n",
      "Cyprus\n",
      "Czechia\n",
      "Côte D'Ivoire\n",
      "DR Congo\n",
      "Denmark\n",
      "Djibouti\n",
      "Dominica\n",
      "Dominican Republic\n",
      "Ecuador\n",
      "Egypt\n",
      "El Salvador\n",
      "Equatorial Guinea\n",
      "Eritrea\n",
      "Estonia\n",
      "Eswatini\n",
      "Ethiopia\n",
      "Fiji\n",
      "Finland\n",
      "France\n",
      "Gabon\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n",
      "Greece\n",
      "Grenada\n",
      "Guatemala\n",
      "Guinea\n",
      "Guinea Bissau\n",
      "Guyana\n",
      "Haiti\n",
      "Honduras\n",
      "Hungary\n",
      "Iceland\n",
      "India\n",
      "Indonesia\n",
      "Iran\n",
      "Iraq\n",
      "Ireland\n",
      "Israel\n",
      "Italy\n",
      "Jamaica\n",
      "Japan\n",
      "Jordan\n",
      "Kazakhstan\n",
      "Kenya\n",
      "Kiribati\n",
      "Kuwait\n",
      "Kyrgyzstan\n",
      "Laos\n",
      "Latvia\n",
      "Lebanon\n",
      "Lesotho\n",
      "Liberia\n",
      "Libya\n",
      "Liechtenstein\n",
      "Lithuania\n",
      "Luxembourg\n",
      "Madagascar\n",
      "Malawi\n",
      "Malaysia\n",
      "Maldives\n",
      "Mali\n",
      "Malta\n",
      "Marshall Islands\n",
      "Mauritania\n",
      "Mauritius\n",
      "Mexico\n",
      "Micronesia\n",
      "Moldova\n",
      "Monaco\n",
      "Mongolia\n",
      "Montenegro\n",
      "Morocco\n",
      "Mozambique\n",
      "Myanmar\n",
      "Namibia\n",
      "Nauru\n",
      "Nepal\n",
      "Netherlands\n",
      "New Zealand\n",
      "Nicaragua\n",
      "Niger\n",
      "Nigeria\n",
      "North Korea\n",
      "North Macedonia\n",
      "Norway\n",
      "Oman\n",
      "Pakistan\n",
      "Palau\n",
      "Panama\n",
      "Papua New Guinea\n",
      "Paraguay\n",
      "Peru\n",
      "Philippines\n",
      "Poland\n",
      "Portugal\n",
      "Qatar\n",
      "Romania\n",
      "Russia\n",
      "Rwanda\n",
      "Saint Kitts and Nevis\n",
      "Saint Lucia\n",
      "Saint Vincent and the Grenadines\n",
      "Samoa\n",
      "San Marino\n",
      "Sao Tome and Principe\n",
      "Saudi Arabia\n",
      "Senegal\n",
      "Serbia\n",
      "Seychelles\n",
      "Sierra Leone\n",
      "Singapore\n",
      "Slovakia\n",
      "Slovenia\n",
      "Solomon Islands\n",
      "Somalia\n",
      "South Africa\n",
      "South Korea\n",
      "South Sudan\n",
      "Spain\n",
      "Sri Lanka\n",
      "Sudan\n",
      "Suriname\n",
      "Sweden\n",
      "Switzerland\n",
      "Syria\n",
      "Tajikistan\n",
      "Tanzania\n",
      "Thailand\n",
      "Timor-Leste\n",
      "Togo\n",
      "Tonga\n",
      "Trinidad and Tobago\n",
      "Tunisia\n",
      "Turkmenistan\n",
      "Tuvalu\n",
      "Uganda\n",
      "Ukraine\n",
      "United Arab Emirates\n",
      "United Kingdom\n",
      "United States of America\n",
      "Uruguay\n",
      "Uzbekistan\n",
      "Vanuatu\n",
      "Venezuela\n",
      "Viet Nam\n",
      "Yemen\n",
      "Zambia\n",
      "Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "def load_country_data(): # -> List[str]\n",
    "    # Wrapper function to create a class, load country data and return a list of countries\n",
    "    loader = CountryLoader()\n",
    "    return loader.load_countries()\n",
    "\n",
    "# Test the loader\n",
    "try:\n",
    "    countries = load_country_data()\n",
    "    print(f'Successfully extracted {len(countries)} countries')\n",
    "    print('\\n'.join(sorted(countries)))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error during testing: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fc90e5-7b3f-4d7d-a895-2a7aa7946230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityLoader:\n",
    "    # Class to load and process 500 most populated cities as per World Population Review\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://worldpopulationreview.com/cities'\n",
    "\n",
    "        # Set up logger to log messages for various events and errors\n",
    "        logging.basicConfig(level=logging.INFO) # Log messages with a security level of INFO or higher\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def fetch_and_parse(self, url: str) -> BeautifulSoup:\n",
    "        # Fetch content from the URL and return parsed BeautifulSoup object\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            self.logger.error(f'Failed to fetch URL {url}: {str(e)}')\n",
    "            raise\n",
    "\n",
    "    def extract_cities(self, soup: BeautifulSoup): # -> List[str]\n",
    "        # Extract city names from parsed HTML\n",
    "        cities = []\n",
    "\n",
    "        try:\n",
    "            # 'my-6' is a unique CSS class, not present elsewhere in the HTML\n",
    "            # This div contains the names of cities inside <a class=\"text-wpr-link\">\n",
    "            block = soup.find('div', class_='my-6') # Works as of 22nd January 2025\n",
    "\n",
    "            if block is None:\n",
    "                self.logger.error(f'No div with class \"my-6\" found, URL {url} structure has likely been changed\"')\n",
    "                                            \n",
    "            for city in block.find_all('a', class_='text-wpr-link'): # Works as of 22nd Janaury 2025\n",
    "                name = city.text.strip()\n",
    "                if name:\n",
    "                    cities.append(name)\n",
    "\n",
    "            # Check if the cities list is populated\n",
    "            if not cities:\n",
    "                self.logger.warning('No city names found in \"my-6\" block')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'An error occured while extracting city names: {str(e)}')\n",
    "            raise\n",
    "\n",
    "        return cities\n",
    "\n",
    "    def load_cities(self): # -> List[str]\n",
    "        # Main method to load and process city data\n",
    "        try:\n",
    "            soup = self.fetch_and_parse(self.base_url)\n",
    "            cities = self.extract_cities(soup)\n",
    "\n",
    "            return cities\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Failed to load cities: {str(e)}')\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d2fa52-676c-4fe8-91b1-b4cd282738a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 500 cities\n",
      "Tokyo\n",
      "Delhi\n",
      "Shanghai\n",
      "Dhaka\n",
      "Sao Paulo\n",
      "Cairo\n",
      "Mexico City\n",
      "Beijing\n",
      "Mumbai\n",
      "Osaka\n"
     ]
    }
   ],
   "source": [
    "def load_city_data(): # -> List[str]\n",
    "    # Wrapper function to create a class, load city data and return a list of cities\n",
    "    loader = CityLoader()\n",
    "    return loader.load_cities()\n",
    "\n",
    "# Test the loader\n",
    "try:\n",
    "    cities = load_city_data()[:500]\n",
    "    print(f'Successfully extracted {len(cities)} cities')\n",
    "    print('\\n'.join(cities[:10])) # Printing 10 city names for testing\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error during testing: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f579ebba-ecec-42b0-95e2-b3337d738c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "def create_graph(places, title, height='750px'):\n",
    "    '''\n",
    "    Crates and visualizes an Atlas game graph using Pyvis\n",
    "\n",
    "    Args:\n",
    "        places: List of place names (countries/cities)\n",
    "        title: Title for the graph visualization\n",
    "        height: Height of the visualization\n",
    "    '''\n",
    "\n",
    "    # Create NetworkX graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(places)\n",
    "\n",
    "    # Add edges based on Atlas rules\n",
    "    for source in places:\n",
    "        last_letter = source[-1].lower()\n",
    "        for target in places:\n",
    "            first_letter = target[0].lower()\n",
    "            if target != source and last_letter == first_letter:\n",
    "                G.add_edge(source, target)\n",
    "\n",
    "    # Convert to Pyvis network\n",
    "    # Visualization spans the entire width of the container taat holds the graph visualization\n",
    "    # Background color of visualization is white\n",
    "    net = Network(height=height, width=\"100%\", directed=True, bgcolor='#ffffff')\n",
    "    net.repulsion()\n",
    "\n",
    "    # Take the NetworkX graph and converts it into a Pyvis-compatible format\n",
    "    net.from_nx(G)\n",
    "\n",
    "    '''\n",
    "    Configure for better visualization\n",
    "    \n",
    "    physics: Dictates how nodes and edges are positioned in the visualization\n",
    "        hierarchicalRepulsion: Organizes nodes in a hierarchical layout while applying forces to avoid overlap\n",
    "        minVelocity: Sets the minimum speed for nodes to stop moving\n",
    "    '''\n",
    "    \n",
    "    net.set_options(\"\"\"\n",
    "    var options = {\n",
    "        \"nodes\": {\n",
    "            \"font\": {\n",
    "                \"size\": 12\n",
    "            }\n",
    "        },\n",
    "        \"edges\": {\n",
    "            \"arrows\": {\n",
    "                \"to\": {\n",
    "                    \"enabled\": true,\n",
    "                    \"scaleFactor\": 0.5\n",
    "                }\n",
    "            },\n",
    "            \"smooth\": {\n",
    "                \"type\": \"continuous\",\n",
    "                \"forceDirection\": \"none\"\n",
    "            }\n",
    "        },\n",
    "        \"physics\": {\n",
    "            \"forceAtlas2Based\": {\n",
    "                \"gravitationalConstant\": -100,\n",
    "                \"centralGravity\": 0.01,\n",
    "                \"springLength\": 200,\n",
    "                \"springConstant\": 0.08,\n",
    "                \"damping\": 0.4,\n",
    "                \"avoidOverlap\": 1\n",
    "            },\n",
    "            \"minVelocity\": 23,\n",
    "            \"solver\": \"forceAtlas2Based\"\n",
    "        }\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    # Set node properties\n",
    "    for node in net.nodes:\n",
    "        '''\n",
    "        Assign a unique colour to each node based on first letter of its name\n",
    "        Extract the first letter of the node's name\n",
    "        Map the hash of the letter to a range of hues (0-359) in the HSL color space\n",
    "        HSLA format: base colour, saturation, lightness (brightness), alpha (transparency)\n",
    "        Result: Nodes with the same starting letter share similar colours\n",
    "        '''\n",
    "        hue = hash(node['id'][0].lower()) % 360\n",
    "        node['color'] = f'hsla({hue}, 70%, 60%, 0.9)'\n",
    "        \n",
    "        in_degree = G.in_degree(node['id'])\n",
    "        out_degree = G.out_degree(node['id'])\n",
    "        node['size'] = 10\n",
    "        \n",
    "        # Add hover information\n",
    "        node['title'] = (\n",
    "            f\"Place: {node['id']}<br>\"\n",
    "            f\"First Letter: {node['id'][0]}<br>\"\n",
    "            f\"Last Letter: {node['id'][-1]}<br>\"\n",
    "            f\"Incoming Moves: {in_degree}<br>\"\n",
    "            f\"Possible Moves: {out_degree}\"\n",
    "        )\n",
    "    \n",
    "    # Set edge properties\n",
    "    for edge in net.edges:\n",
    "        edge['color'] = {'color': '#848484', 'opacity': 0.6}\n",
    "        edge['width'] = 1\n",
    "    \n",
    "    # Save the visualization\n",
    "    fileName = f'atlas_{title.lower().replace(' ', '_')}.html'\n",
    "    net.show(fileName)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def print_graph_stats(G, name):\n",
    "    # Print basic statistics about the graph\n",
    "    print(f'Stats for {name}:')\n",
    "    print(f'Number of places: {G.number_of_nodes()}')\n",
    "    print(f'Number of possible moves: {G.number_of_edges()}')\n",
    "    print(f'Average moves per place: {G.number_of_edges() / G.number_of_nodes():.2f}')\n",
    "    \n",
    "    # Find places with most and least moves\n",
    "    out_degrees = dict(G.out_degree())\n",
    "    max_moves = max(out_degrees.items(), key=lambda x: x[1])\n",
    "    min_moves = min(out_degrees.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f'Most possible moves: {max_moves[0]} ({max_moves[1]} moves)')\n",
    "    print(f'Least possible moves: {min_moves[0]} ({min_moves[1]} moves)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c72649-9819-4f24-98b3-1d6ebfc246da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for Countries Graph:\n",
      "Number of places: 192\n",
      "Number of possible moves: 2001\n",
      "Average moves per place: 10.42\n",
      "Most possible moves: Bahamas (26 moves)\n",
      "Least possible moves: Burkina Faso (1 moves)\n"
     ]
    }
   ],
   "source": [
    "# Create and visualize all three graphs\n",
    "countries = countries\n",
    "\n",
    "country_graph = create_graph(places=countries, title='Countries')\n",
    "print_graph_stats(country_graph, 'Countries Graph')\n",
    "\n",
    "# city_graph = create_graph(places=cities, title='Cities')\n",
    "# print_graph_stats(city_graph, 'Cities Graph')\n",
    "\n",
    "# combined_graph = create_graph(places=countries + cities, title='Combined')\n",
    "# print_graph_stats(combined_graph, 'Combined Graph')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
